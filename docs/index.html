<html lang="en-US">
  <head>
    <meta charset="UTF-8">

    <title>Learning a Generative Transition Model for Uncertainty-Aware Robotic Manipulation</title>
    <meta property="og:title" content="Learning a Generative Transition Model for Uncertainty-Aware Robotic Manipulation">
    <meta property="og:locale" content="en_US">
    <meta name="description" content="Learning a Generative Transition Model for Uncertainty-Aware Robotic Manipulation">
    <meta property="og:description" content="Learning a Generative Transition Model for Uncertainty-Aware Robotic Manipulation">
    <link rel="canonical" href="https://pantor.github.io/learning-transition-model-for-manipulation/">
    <meta property="og:url" content="https://pantor.github.io/learning-transition-model-for-manipulation/">
    <meta property="og:site_name" content="learning-transition-model-for-manipulation">
    <script type="application/ld+json">
      {"headline":"Learning a Generative Transition Model for Robotic Grasping","@type":"WebSite","url":"https://pantor.github.io/learning-transition-model-for-manipulation/","name":"learning-transition-model-for-manipulation","description":"Learning a Generative Transition Model for Robotic Manipulation","@context":"http://schema.org"
    }
    </script>

    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="theme-color" content="#157878">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <link rel="stylesheet" href="style.css">
  </head>

  <body>
    <div class="main-content" style="display: flex; justify-content: space-between; flex-wrap: wrap;">
      <img src="assets/logos/logo-kit.png" height="82em" />
      <img src="assets/logos/logo-ipr.png" height="82em" />
    </div>

    <header class="page-header" role="banner">
      <h1 class="project-name">Learning a Generative Transition Model for Uncertainty&#8209;Aware Robotic&nbsp;Manipulation</h1>
      <h2 class="project-tagline">
        Lars&nbsp;Berscheid, Pascal&nbsp;MeiÃŸner, and Torsten&nbsp;KrÃ¶ger<br>
        <a href="https://ipr.kit.edu" style="color: white;">Intelligent&nbsp;Process&nbsp;Automation and Robotics&nbsp;Lab&nbsp;(IPR), Karlsruhe&nbsp;Institute of&nbsp;Technology&nbsp;(KIT)</a>
      </h2>

      <a class="btn" disabled>View Paper</a>
      <a href="https://drive.google.com/open?id=1zpQiE33D4JNLQ1kD8PeW4H1DP4ddwY1a" class="btn">View Video</a>
      <a href="https://github.com/pantor/learning-transition-model-for-manipulation" class="btn">View Code</a>
    </header>

    <main id="content" class="main-content" role="main">
      <center>
        <h4>Abstract</h4>
        <p class="text">
          Planning within robotic tasks usually involves a transition model of the environmentâ€™s state, which are often in a high-dimensional visual space. We explicitly learn a transition model using the Bicycle GAN architecture, capturing the stochastic process of the real world and thus allowing for uncertainty estimation. Then, we apply this approach to the real-world task of bin picking, where a robot should empty a bin by grasping and shifting objects as fast as possible. The model is trained with around \num{30000} pairs of real-world images before and after a given object manipulation. Such a model allows for two important skills: First, for applications with flange-mounted cameras, the picks per hours can be increased by skipping the recording of images. Second, we use the model for planning ahead while emptying the bin, minimizing either the number of actions or maximizing the estimated reward over the next $N$ actions. We show both advantages with real-robot experiments and set a new state-of-the-art result in the YCB block test.
        </p>
  
        <h4>Conference Video</h4>
        <div class="videobereich">
          <iframe class="videoextern" src="https://drive.google.com/file/d/1zpQiE33D4JNLQ1kD8PeW4H1DP4ddwY1a/preview" width="640" height="480" frameborder="0" allowfullscreen></iframe>
        </div>
      </center>
      

      <hr style="margin: 5em; margin-top: 6em;">

      <h2>Supplementary Material</h2>
      
      <p class="text">
        Below, we present supplementary material for our publication submitted to ICRA 2020.
        This includes more visual examples of the transition model (as predicted depth images with corresponding uncertainties), more videos and more detailed experimental results.
        We follow the structure of the experimental evaluation of the paper and the conference video above.
      </p>
       

      <h3>1. Iterative Prediction</h3>
      
      <p class="text">
        As a reminder, we focus on the task of bin picking using a flange-mounted depth camera.
        The robot should empty the bin using grasping and pre-grasping manipulation (like shifting and pushing) as fast as possible.
        The transition model predicts the depth image after a given action and its corresponding reward.
        It was trained on around 30000 pairs of images before and after a manipulation action, using a dataset with three different object types.
        By applying the transition model iteratively, the robot is able to plan multiple actions based on a single image.
      </p>

      <h4>1.1 Simple Example</h4>
      <steps-carousel name="simple-example-1" v-bind:steps="simple_example_1"></steps-carousel>


      <h4>1.2 Complex Manipulation (<a href="https://drive.google.com/open?id=1_C-UF_C_8Q4CJlExdXh57CzIQU20LHtY">Video</a>)</h4>
      <steps-carousel name="complex-manipulation-1" v-bind:steps="complex_manipulation_1"></steps-carousel>


      <h4>1.3 Bin Picking</h4>
      <steps-carousel name="bin-picking-2" v-bind:steps="bin_picking_2"></steps-carousel>


      <h3>2. Grasp Rate</h3>

      <p class="text">
        The grasp rate, given by the percentage of successful grasps, depends on various factors.
        We investigated the dependency of the grasp rate on the prediction step.
        The experiment used the bin picking scenarious from above.
      </p>

      <p class="experiment-results">
        Detailed experimental results of each grasp (N = 1080) can be found <a href="detail/grasp-rate.html">here</a>.
      </p>


      <h3 id="ycb-block-test-benchmark">3. YCB Blocks Benchmark</h2>

      <p class="text">
        The <a href="http://www.ycbbenchmarks.com/protocols-and-benchmarks/">YCB Blocks Benchmark</a> is about grasping and placing as many objects as possible into another bin within 2 minutes.
        The wooden cubes have a side length of 2.5cm.
        The gripper needs to be above the other bin before dropping. 
        The robotic benchmark - in contrast to the original medical test - allows for grasping multiple objects at once.
      </p>
      
      <center>
        <img src="assets/block-test.gif" />
      </center>
      
      
      <center>
        <table style="margin-top: 2em; margin-bottom: 2em;">
          <thead>
            <tr>
              <th>Method</th>
              <th>Objects</th>
              <th>Grasp Rate [%]</th>
              <th>Picks Per Hour (PPH)</th>
              <th>Video</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td>Random</td>
              <td style="text-align: right">2.2 Â± 0.7</td>
              <td style="text-align: right">13 Â± 4</td>
              <td style="text-align: right">66 Â± 20</td>
              <td style="text-align: right"><a href="https://drive.google.com/open?id=1OO6sl0XW7w1Caw4n2WMRxDPlo2tv36SQ">1 Object ðŸ”—</a></td>
            </tr>
            <tr>
              <td>Single</td>
              <td style="text-align: right">12.8 Â± 0.3</td>
              <td style="text-align: right">97 Â± 2</td>
              <td style="text-align: right">384 Â± 10</td>
              <td style="text-align: right"><a href="https://drive.google.com/open?id=1nM04orizO_9r_hvEoq3BkA812kTYnFzo">13 Objects ðŸ”—</a></td>
            </tr>
            <tr>
              <td>Single + Prediction</td>
              <td style="text-align: right">16.4 Â± 0.5</td>
              <td style="text-align: right">94 Â± 2</td>
              <td style="text-align: right">492 Â± 14</td>
              <td style="text-align: right"><a href="https://drive.google.com/open?id=1pn0IlyM1ND6t1zs2kNFoWa2VQBZ4tB3M">15 Objects ðŸ”—</a></td>
            </tr>
            <tr>
              <td>Multiple</td>
              <td style="text-align: right">20.4 Â± 1.0</td>
              <td style="text-align: right">96 Â± 2</td>
              <td style="text-align: right">612 Â± 29</td>
              <td style="text-align: right"><a href="https://drive.google.com/open?id=1fwwd4ca5Z-UxAccsfPeQ92qdTENn36mu">20 Objects ðŸ”—</a></td>
            </tr>
            <tr>
              <td>Multiple + Prediction</td>
              <td style="text-align: right">23.4 Â± 0.3</td>
              <td style="text-align: right">94 Â± 2</td>
              <td style="text-align: right">702 Â± 14</td>
              <td style="text-align: right"><a href="https://drive.google.com/open?id=1jsqJQZKdSdHt-12n_03bDCB6ssnA97YJ">26 Objects ðŸ”—</a></td>
            </tr>
          </tbody>
        </table>
      </center>
      

      <p class="experiment-results">
        Detailed experimental results of each try can be found <a href="detail/block-test.html">here</a>.
      </p>


      <h3>4. Planning Ahead</h2>

      <!--h4>Optimize for Fewest Actions</h4-->

      <center>
        <img src="assets/planning-configuration-sm.jpg" style="max-width: 640px;" />
        <p>
          <b>Fig. 2:</b> We showed for this example configuration of three cubes in a row, the robot is able to reduce the average number of actions by 30% by planning a few steps ahead.
        </p>
      </center>

      <h4>4.1 Example for 4 actions</h5>
      <div class="videobereich">
        <iframe class="videoextern" src="https://drive.google.com/file/d/1HercG5puXAsyX0PJzz8_ScoR6ZEIqOT8/preview" width="640" height="480" frameborder="0" allowfullscreen></iframe>
      </div>

      <steps-carousel name="push-try-1" v-bind:steps="push_try_1"></steps-carousel>


      <h4>4.2 Example for 5 actions</h5>
      <div class="videobereich">
        <iframe class="videoextern" src="https://drive.google.com/file/d/1z-LVGtoKAoKiclZW1Y2CVtvJshnLMm-J/preview" width="640" height="480" frameborder="0" allowfullscreen></iframe>
      </div>
      
      <steps-carousel name="push-try-3" v-bind:steps="push_try_3"></steps-carousel>
      

      <!--h4>Optimize for Sum of Rewards</h4-->
  
      <footer class="site-footer">
        <span class="site-footer-credits">
          To explore our work even further:
          <ul>
            <li><a href="https://techxplore.com/news/2019-08-algorithm-robots-pre-grasping-strategies.html">A TechXplore article about pre-grasping manipulation</a></li>
            <li><a href="https://en.ids-imaging.com/casestudies-detail/en_seen-stored-learned.html">An IDS article about our work from an industrial perspective</a></li>
            <li><a href="https://ipr.kit.edu">Our research group at IPR / KIT.</a></li>
          </ul>
        </span>
      </footer>
    </main>

    <footer class="page-header" role="banner">
      <span class="site-footer-owner">Lars Berscheid</span>

      <p>
        Intelligent Process Automation and Robotics Lab (IPR)<br>
        Karlsruhe Institute of Technology (KIT)<br>
        2019
      </p>
    </footer>
    
    <!-- Templates -->
    <script type="text/x-template" id="steps-carousel-template">
      <div>
        <div style="display: flex; align-items: center; flex-wrap: nowrap; overflow-y: scroll;">
          <div v-for="i in steps.length" ref="steps" style="min-width: 300px; margin: 5px;">
            <p>
              <b>Step {{ i - 1 }}</b> <small v-if="i - 1 == 0">(Image Measurement)</small><br>
              Reward: {{ steps[i - 1].reward }}<br>
              Estimation: {{ steps[i - 1].estimated_reward }} <span v-if="steps[i - 1].estimated_reward_std">Â± {{ steps[i - 1].estimated_reward_std }}</span><br>
              Action:
              <span v-if="steps[i - 1].action_type < 3">Grasp (Type {{ steps[i - 1].action_type }})</span>
              <span v-if="steps[i - 1].action_type == 3">Shift</span>
              <span v-if="steps[i - 1].action_type == 4">Bin Empty</span>
            </p>
  
            <div v-bind:class="current_step == i-1 ? 'highlight-image' : ''">
              <img :src="`assets/${name}/result-${i - 1}.jpg`" v-on:click="setStep(i - 1, steps.length)"/>
              <img :src="`assets/${name}/uncertainty-${i - 1}.jpg`" v-on:click="setStep(i - 1, steps.length)"/>
            </div>
          </div>
        </div>
  
        <div style="display: flex; justify-content: space-between">
          <div>
            <button class="button-left chevron left" v-on:click="setStep(current_step - 1, steps.length)" :disabled="current_step <= 0"></button>
          </div>
        
          <div>
            <button class="button-right chevron right" v-on:click="setStep(current_step + 1, steps.length)" :disabled="current_step >= steps.length - 1"></button>
          </div>
        </div>
      </div>
    </script>

    <script src="https://cdn.jsdelivr.net/npm/vue/dist/vue.js"></script>
    <script src="https://code.jquery.com/jquery-3.4.1.min.js" integrity="sha256-CSXorXvZcTkaix6Yvo6HppcZGetbYMGWSFlBw8HfCJo=" crossorigin="anonymous"></script>
		
    <script src="script.js"></script>
  </body>
</html>
