<html lang="en-US">
  <head>
    <meta charset="UTF-8">

    <title>Learning a Generative Transition Model for Uncertainty-Aware Robotic Manipulation</title>
    <meta property="og:title" content="Learning a Generative Transition Model for Uncertainty-Aware Robotic Manipulation">
    <meta property="og:locale" content="en_US">
    <meta name="description" content="Learning a Generative Transition Model for Uncertainty-Aware Robotic Manipulation">
    <meta property="og:description" content="Learning a Generative Transition Model for Uncertainty-Aware Robotic Manipulation">
    <link rel="canonical" href="https://pantor.github.io/learning-transition-model-for-manipulation/">
    <meta property="og:url" content="https://pantor.github.io/learning-transition-model-for-manipulation/">
    <meta property="og:site_name" content="learning-transition-model-for-manipulation">
    <script type="application/ld+json">
      {"headline":"Learning a Generative Transition Model for Robotic Grasping","@type":"WebSite","url":"https://pantor.github.io/learning-transition-model-for-manipulation/","name":"learning-transition-model-for-manipulation","description":"Learning a Generative Transition Model for Robotic Manipulation","@context":"http://schema.org"
    }
    </script>

    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="theme-color" content="#157878">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <link rel="stylesheet" href="style.css">
  </head>

  <body>
    <div class="main-content" style="display: flex; justify-content: space-between; flex-wrap: wrap;">
      <img src="assets/logos/logo-kit.png" height="82em" />
      <img src="assets/logos/logo-ipr.png" height="82em" />
    </div>

    <header class="page-header" role="banner">
      <h1 class="project-name">Learning a Generative Transition Model for Uncertainty&#8209;Aware Robotic&nbsp;Manipulation</h1>
      <h2 class="project-tagline">
        Lars&nbsp;Berscheid, Pascal&nbsp;MeiÃŸner, and Torsten&nbsp;KrÃ¶ger<br>
        <a href="https://ipr.kit.edu" style="color: white;">Intelligent&nbsp;Process&nbsp;Automation and Robotics&nbsp;Lab&nbsp;(IPR), Karlsruhe&nbsp;Institute of&nbsp;Technology&nbsp;(KIT)</a>
      </h2>

      <a href="https://drive.google.com/file/d/1JDOeTVgLxhnPdZ3bylB2vNiiSRr2-69p/view?usp=sharing" class="btn" disabled>View Paper (Draft)</a>
      <a href="https://drive.google.com/open?id=1h5FS_Q2BolOuQupU4NfN2wpcbErKNix9" class="btn">View Video</a>
      <a href="https://github.com/pantor/learning-transition-model-for-manipulation" class="btn">View Code</a>
    </header>

    <main id="content" class="main-content" role="main">
      <center>
        <h3>Abstract</h3>
        <p class="text">
	  Robot learning of real-world manipulation tasks remains challenging and time consuming, even though actions are often simplified by task-specific manipulation primitives.
	  To offset the removed time dependency, we additionally learn a visual transition model able to capture stochastic predictions and thus allowing for uncertainty estimation.
	  We apply this approach to bin picking, where a robot should empty a bin using grasping as well as pre-grasping manipulation as fast as possible.
	  The transition model is trained with around 34000 pairs of real-world images before and after a manipulation action.
	  Our approach enables two important skills: First, for applications with flange-mounted cameras, picks per hours (PPH) can be increased by skipping image measurements by around 15%.
	  Second, we use the model to plan action sequences ahead of time and optimize time-dependent rewards, e.g. to minimize the number of actions required to empty the bin.
	  We demonstrate both advantages with real-robot experiments and set a new state-of-the-art result in the Box and Blocks Test with 702Â±14 PPH.
	</p>
  
        <h3>Conference Video</h3>
        <div class="videobereich">
          <iframe class="videoextern" src="https://drive.google.com/file/d/1h5FS_Q2BolOuQupU4NfN2wpcbErKNix9/preview" width="640" height="480" frameborder="0" allowfullscreen></iframe>
        </div>
      </center>
      

      <hr style="margin: 5em; margin-top: 6em;">


      <h2>Supplementary Material</h2>
      <p class="text">
        Below, we present supplementary material for our publication submitted to ICRA 2020.
        This includes more visual examples of the transition model (as predicted depth images with corresponding uncertainties), more videos and more detailed experimental results.
        We follow the structure of the experimental evaluation in the paper and the conference video above.
      </p>
       

      <h3>1. Iterative Prediction</h3>
      <p class="text">
        To recap the paper's introduction, we focus on the task of bin picking using a flange-mounted depth camera.
        The robot should empty the bin using grasping and pre-grasping manipulation (like shifting and pushing) as fast as possible.
        We proposed and implemented a transition model, which is able to predict the depth image after a given action and its corresponding reward.
        It was trained on around 34000 pairs of real-world images before and after a manipulation action, using a dataset with three different object types.
        By applying the transition model iteratively, the robot is then able to plan multiple actions based on a single image.
      </p>

      <p class="text">
        Below, the predicted images of the three examples from the conference video are shown.
        We denote step 0 as the measured image with zero uncertainty; all following images are predictions.
        To each step, the measured reward after execution, the estimated reward and its propagated uncertainty before execution and the action type is given.
        We use three grasping actions with different pre-shaped gripper widths (type 0, 1, and 2), one shifting action (type 3) and a bin empty action (type 4).
        Within the depth images (top) and the uncertainty image (bottom), the gripper is sketched (white) with a bounding rectangle (red).
        The uncertainty is shown from low (blue) to high (red).
      </p>

      <h4>1.1 Simple Example</h4>
      <steps-carousel name="simple-example-1" v-bind:steps="simple_example_1"></steps-carousel>


      <h4>1.2 Complex Manipulation</h4>
      <steps-carousel name="complex-manipulation-1" v-bind:steps="complex_manipulation_1"></steps-carousel>


      <h4>1.3 Bin Picking</h4>
      <steps-carousel name="bin-picking-2" v-bind:steps="bin_picking_2"></steps-carousel>


      <h3>2. Grasp Rate</h3>

      <p class="text">
        Let the grasp rate be the average percentage of successful grasps.
        We investigated the dependency of the grasp rate on the prediction step in a bin picking scenario.
        Measuring 1080 grasp attempts, we find that even after the third prediction step the grasp rate is still over 90%.
        A graphical evaluation is shown in the paper.
      </p>

      <p class="experiment-results">
        Detailed experimental results of each grasp (N = 1080) can be found <a href="detail/grasp-rate.html">here</a>.
      </p>


      <h3 id="ycb-block-test-benchmark">3. YCB Box and Blocks Benchmark</h2>

      <p class="text">
        The <a href="http://www.ycbbenchmarks.com/protocols-and-benchmarks/">YCB Box and Blocks Benchmark</a> is about grasping and placing as many objects as possible into another bin within 2 minutes.
        The wooden cubes have a side length of 2.5cm; the gripper needs to be above the other bin before dropping. 
        The robotic benchmark - in contrast to the original medical test - allows for grasping multiple objects at once.
        We evaluate different settings, partially where the robot has learned to grasp multiple objects using semantic grasping.
        Combining this with the transition model, we achieved up to <b>26 objects</b> in <b>2 mins</b>.
      </p>
      
      <center>
        <img src="assets/block-test.gif" />
      </center>
      <p class="text" style="margin-bottom: 3em;">
        <b>Fig. 1:</b> Our setup for the YCB Box and Blocks Benchmark.
        Note that the Franka Panda robotic arm has a max. velocity of 1.7 m/s and max. acceleration of 13 m/sÂ², leavingr room for improvement by using a faster robot.
      </p>

      <p class="text">
        <b>Tab. 1:</b> The summarized results of the YCB Box and Blocks Benchmark.
        We evaluated two different settings:
        First, single or multiple denotes if the robot was trained to grasp multiple objects at once via semantic grasping.
        Second, prediction uses the transition model to skip some image mesaurements.
        The random grasp rate denotes uniformly sampled grasps within the bounding box.
      </p>
      
      
      <table style="margin-top: 2em; margin-bottom: 2em;">
        <thead>
          <tr>
            <th>Method</th>
            <th>Objects</th>
            <th>Grasp Rate [%]</th>
            <th>Picks Per Hour (PPH)</th>
            <th>Video</th>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td>Random</td>
            <td style="text-align: right">2.2 Â± 0.7</td>
            <td style="text-align: right">13 Â± 4</td>
            <td style="text-align: right">66 Â± 20</td>
            <td style="text-align: right"><a href="https://drive.google.com/open?id=1OO6sl0XW7w1Caw4n2WMRxDPlo2tv36SQ">1 Object ðŸ”—</a></td>
          </tr>
          <tr>
            <td>Single</td>
            <td style="text-align: right">12.8 Â± 0.3</td>
            <td style="text-align: right">97 Â± 2</td>
            <td style="text-align: right">384 Â± 10</td>
            <td style="text-align: right"><a href="https://drive.google.com/open?id=1nM04orizO_9r_hvEoq3BkA812kTYnFzo">13 Objects ðŸ”—</a></td>
          </tr>
          <tr>
            <td>Single + Prediction</td>
            <td style="text-align: right">16.4 Â± 0.5</td>
            <td style="text-align: right">94 Â± 2</td>
            <td style="text-align: right">492 Â± 14</td>
            <td style="text-align: right"><a href="https://drive.google.com/open?id=1pn0IlyM1ND6t1zs2kNFoWa2VQBZ4tB3M">15 Objects ðŸ”—</a></td>
          </tr>
          <tr>
            <td>Multiple</td>
            <td style="text-align: right">20.4 Â± 1.0</td>
            <td style="text-align: right">96 Â± 2</td>
            <td style="text-align: right">612 Â± 29</td>
            <td style="text-align: right"><a href="https://drive.google.com/open?id=1fwwd4ca5Z-UxAccsfPeQ92qdTENn36mu">20 Objects ðŸ”—</a></td>
          </tr>
          <tr>
            <td>Multiple + Prediction</td>
            <td style="text-align: right">23.4 Â± 0.3</td>
            <td style="text-align: right">94 Â± 2</td>
            <td style="text-align: right">702 Â± 14</td>
            <td style="text-align: right"><a href="https://drive.google.com/open?id=1jsqJQZKdSdHt-12n_03bDCB6ssnA97YJ">26 Objects ðŸ”—</a></td>
          </tr>
        </tbody>
      </table>

      <p class="experiment-results">
        Detailed experimental results of each try can be found <a href="detail/block-test.html">here</a>.
      </p>


      <h3>4. Planning Ahead</h3>
      <p class="text">
        We've implemented a breadth-first planning tree for optimizing multi-step cost functions.
        Below, we demonstrate the planning capability at a experiment, where the robot optimizes for fewest actions for emptying the bin.
        Videos and image predictions are shown for two typical approaches - using a different number of steps - for emptying the bin.
        By planning ahead, the robot more often starts by shifting a middle cube, thus reducing the average number of actions from 4.8 to 4.1 steps.
      </p>

      <!--h4>Optimize for Fewest Actions</h4-->

      <img src="assets/planning-configuration-sm.jpg" style="width: 100%; display: block;" />
      <p>
        <b>Fig. 2:</b> For this example configuration of three cubes in a row, the robot is able to reduce the average number of actions by 0.7 steps by planning a few steps ahead.
      </p>

      <h4>4.1 Example for 4 actions</h4>
      <div class="videobereich">
        <iframe class="videoextern" src="https://drive.google.com/file/d/1HercG5puXAsyX0PJzz8_ScoR6ZEIqOT8/preview" width="640" height="480" frameborder="0" allowfullscreen></iframe>
      </div>

      <steps-carousel name="push-try-1" v-bind:steps="push_try_1"></steps-carousel>


      <h4>4.2 Example for 5 actions</h4>
      <div class="videobereich">
        <iframe class="videoextern" src="https://drive.google.com/file/d/1z-LVGtoKAoKiclZW1Y2CVtvJshnLMm-J/preview" width="640" height="480" frameborder="0" allowfullscreen></iframe>
      </div>
      
      <steps-carousel name="push-try-3" v-bind:steps="push_try_3"></steps-carousel>
      

      <!--h4>Optimize for Sum of Rewards</h4-->
  
      <footer class="site-footer">
        <span class="site-footer-credits">
          To explore our work even further:
          <ul>
            <li><a href="https://techxplore.com/news/2019-08-algorithm-robots-pre-grasping-strategies.html">A TechXplore article about pre-grasping manipulation</a></li>
            <li><a href="https://en.ids-imaging.com/casestudies-detail/en_seen-stored-learned.html">An IDS article about our work from an industrial perspective</a></li>
            <li><a href="https://ipr.kit.edu">Our research group at IPR / KIT.</a></li>
          </ul>
        </span>
      </footer>
    </main>

    <footer class="page-header" role="banner">
      <span class="site-footer-owner">Lars Berscheid</span>

      <p>
        Intelligent Process Automation and Robotics Lab (IPR)<br>
        Karlsruhe Institute of Technology (KIT)<br>
        &copy; 2019
      </p>
    </footer>
    
    <!-- Templates -->
    <script type="text/x-template" id="steps-carousel-template">
      <div>
        <div style="display: flex; align-items: center; flex-wrap: nowrap; overflow-y: scroll;">
          <div v-for="i in steps.length" ref="steps" style="min-width: 300px; margin: 5px;">
            <p>
              <b>Step {{ i - 1 }}</b> <small v-if="i - 1 == 0">(Image Measurement)</small><br>
              Reward: {{ steps[i - 1].reward }}<br>
              Estimation: {{ steps[i - 1].estimated_reward }} <span v-if="steps[i - 1].estimated_reward_std">Â± {{ steps[i - 1].estimated_reward_std }}</span><br>
              Action:
              <span v-if="steps[i - 1].action_type < 3">Grasp (Type {{ steps[i - 1].action_type }})</span>
              <span v-if="steps[i - 1].action_type == 3">Shift</span>
              <span v-if="steps[i - 1].action_type == 4">Bin Empty</span>
            </p>
  
            <div v-bind:class="current_step == i-1 ? 'highlight-image' : ''">
              <img :src="`assets/${name}/result-${i - 1}.jpg`" v-on:click="setStep(i - 1, steps.length)"/>
              <img :src="`assets/${name}/uncertainty-${i - 1}.jpg`" v-on:click="setStep(i - 1, steps.length)"/>
            </div>
          </div>
        </div>
  
        <div style="display: flex; justify-content: space-between">
          <div>
            <button class="button-left chevron left" v-on:click="setStep(current_step - 1, steps.length)" :disabled="current_step <= 0"></button>
          </div>
        
          <div>
            <button class="button-right chevron right" v-on:click="setStep(current_step + 1, steps.length)" :disabled="current_step >= steps.length - 1"></button>
          </div>
        </div>
      </div>
    </script>

    <script src="https://cdn.jsdelivr.net/npm/vue/dist/vue.min.js"></script>
    <script src="https://code.jquery.com/jquery-3.4.1.min.js" integrity="sha256-CSXorXvZcTkaix6Yvo6HppcZGetbYMGWSFlBw8HfCJo=" crossorigin="anonymous"></script>
		
    <script src="script.js"></script>
  </body>
</html>
